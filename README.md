# week3summary
Week 3 - Supervised Machine Learning models 
Supervised Learning:
Supervised learning uses labeled data â€” meaning the input data comes with known outputs (targets). The model learns to map inputs to the correct output.
âœ… Used for: Classification and Regression problems.
ðŸ§  Example: Predicting house prices based on size, location, etc.

In Week 3, I deepened my understanding of Supervised Machine Learning models, with a particular focus on regression techniques. Key topics and takeaways included:

Data Preprocessing: Learned the importance of handling categorical data (e.g., label encoding, one-hot encoding), feature scaling, and splitting data into training and test sets.

Regression Models:
Simple Linear Regression: Applied to problems with one independent variable.
Multiple Linear Regression: Extended to multiple predictors, capturing more complex relationships.
Ridge & ElasticNet Regression: Explored regularization techniques to handle overfitting and multicollinearity.

Model Evaluation: Gained hands-on experience using metrics like RÂ² score and tools such as GridSearchCV for hyperparameter tuning and cross-validation.

Advanced Models: Implemented more robust models including Decision Tree, Random Forest, SVR, and XGBoost.

Automation: Built pipelines using scikit-learn to streamline preprocessing and model training.

To reinforce my learning, I completed a comprehensive Python script that compares these models using a real-world dataset ("50_Startups.csv"), helping me analyze performance across various algorithms.
